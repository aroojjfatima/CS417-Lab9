{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eId7kSBIvDBk",
        "outputId": "adee3897-7cd6-4cac-f4a0-1bdb51270cf4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 26 20:06:46 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "# Output would be True if Pytorch is using GPU otherwise it would be False."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNnt9w02vK1f",
        "outputId": "cf92d98c-9738-432f-f5de-2d3e8d17f23f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "# Standard output is '/device:GPU:0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ttIEmxHHvXYf",
        "outputId": "32039651-279c-494c-cb44-ef9cfd29af39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4RQRVU_vYTN",
        "outputId": "4321e31b-5f8c-410b-e791-a26911df1460"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pludz8awvcnh",
        "outputId": "c5667201-176e-47ce-cc6d-8a64dc97dfae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-d48okd4k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-d48okd4k\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4293 sha256=c82ce33b0b382863bcde2d5a6d6b17078e6b837ed51e2de42acad9809d30f3be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kviiot7s/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlL6sFEVvfaq",
        "outputId": "977a6855-a388-41d9-e9e1-70f1a1259396"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJyvl_S0sL6G",
        "outputId": "d3eea488-1ea5-43fa-9339-49407b371add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time (shared memory): 73.7815 ms\n",
            "CPU execution time: 7519.55 ms\n",
            "Results are correct!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "\n",
        "#define TILE_WIDTH 16\n",
        "\n",
        "// CUDA Kernel using Shared Memory\n",
        "__global__ void MatrixMulShared(float *d_M, float *d_N, float *d_P, int Width) {\n",
        "    __shared__ float Mds[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int bx = blockIdx.x; int by = blockIdx.y;\n",
        "    int tx = threadIdx.x; int ty = threadIdx.y;\n",
        "\n",
        "    int Row = by * TILE_WIDTH + ty;\n",
        "    int Col = bx * TILE_WIDTH + tx;\n",
        "    float Pvalue = 0;\n",
        "\n",
        "    for (int m = 0; m < (Width-1)/TILE_WIDTH+1; ++m) {\n",
        "        if (Row < Width && m*TILE_WIDTH+tx < Width)\n",
        "            Mds[ty][tx] = d_M[Row*Width + m*TILE_WIDTH + tx];\n",
        "        else\n",
        "            Mds[ty][tx] = 0.0;\n",
        "        if (Col < Width && m*TILE_WIDTH+ty < Width)\n",
        "            Nds[ty][tx] = d_N[(m*TILE_WIDTH + ty)*Width + Col];\n",
        "        else\n",
        "            Nds[ty][tx] = 0.0;\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < TILE_WIDTH; ++k)\n",
        "            Pvalue += Mds[ty][k] * Nds[k][tx];\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (Row < Width && Col < Width)\n",
        "        d_P[Row*Width+Col] = Pvalue;\n",
        "}\n",
        "\n",
        "// CPU Matrix Multiplication\n",
        "void MatrixMulCPU(float *M, float *N, float *P, int Width) {\n",
        "    for (int i = 0; i < Width; i++)\n",
        "        for (int j = 0; j < Width; j++) {\n",
        "            float sum = 0;\n",
        "            for (int k = 0; k < Width; k++)\n",
        "                sum += M[i * Width + k] * N[k * Width + j];\n",
        "            P[i * Width + j] = sum;\n",
        "        }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int Width = 1024;  // Matrix width\n",
        "    size_t size = Width * Width * sizeof(float);\n",
        "    float *h_M, *h_N, *h_P, *h_P_cpu;\n",
        "\n",
        "    // Allocate and initialize host matrices\n",
        "    h_M = (float *)malloc(size);\n",
        "    h_N = (float *)malloc(size);\n",
        "    h_P = (float *)malloc(size);\n",
        "    h_P_cpu = (float *)malloc(size);\n",
        "\n",
        "    // Initialize matrices with values\n",
        "    // Initialize matrix A with random values\n",
        "for (int i = 0; i < Width; i++) {\n",
        "    for (int j = 0; j < Width; j++) {\n",
        "        h_M[i * Width + j] = (float)(rand() % 100); // Random numbers between 0 and 99\n",
        "    }\n",
        "}\n",
        "\n",
        "// Initialize matrix B with random values\n",
        "for (int i = 0; i < Width; i++) {\n",
        "    for (int j = 0; j < Width; j++) {\n",
        "        h_N[i * Width + j] = (float)(rand() % 100); // Random numbers between 0 and 99\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "    // Allocate device matrices\n",
        "    float *d_M, *d_N, *d_P;\n",
        "    cudaMalloc(&d_M, size);\n",
        "    cudaMalloc(&d_N, size);\n",
        "    cudaMalloc(&d_P, size);\n",
        "\n",
        "    // Copy matrices to device\n",
        "    cudaMemcpy(d_M, h_M, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_N, h_N, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the GPU Kernel\n",
        "    dim3 dimGrid((Width-1)/TILE_WIDTH+1, (Width-1)/TILE_WIDTH+1, 1);\n",
        "    dim3 dimBlock(TILE_WIDTH, TILE_WIDTH, 1);\n",
        "\n",
        "    // Measuring execution time for GPU\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    MatrixMulShared<<<dimGrid, dimBlock>>>(d_M, d_N, d_P, Width);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    std::cout << \"GPU execution time (shared memory): \" << milliseconds << \" ms\\n\";\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(h_P, d_P, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // CPU Matrix Multiplication for verification\n",
        "    auto start_cpu = std::chrono::high_resolution_clock::now();\n",
        "    MatrixMulCPU(h_M, h_N, h_P_cpu, Width);\n",
        "    auto stop_cpu = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<float, std::milli> cpu_duration = stop_cpu - start_cpu;\n",
        "    std::cout << \"CPU execution time: \" << cpu_duration.count() << \" ms\\n\";\n",
        "\n",
        "    // Compare CPU and GPU results\n",
        "\n",
        "    // Compare CPU and GPU results\n",
        "    bool result_correct = true;\n",
        "    for (int i = 0; i < Width * Width; i++) {\n",
        "        if (fabs(h_P_cpu[i] - h_P[i]) > 1e-5) {\n",
        "            result_correct = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "    if (result_correct)\n",
        "        std::cout << \"Results are correct!\" << std::endl;\n",
        "    else\n",
        "        std::cout << \"Results are incorrect!\" << std::endl;\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_M);\n",
        "    cudaFree(d_N);\n",
        "    cudaFree(d_P);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_M);\n",
        "    free(h_N);\n",
        "    free(h_P);\n",
        "    free(h_P_cpu);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ]
}